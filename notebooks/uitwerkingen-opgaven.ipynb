{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uitwerkingen opgaven data-analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opgave 1.3\n",
    "\n",
    "We maken hier gebruik van de error function, gedefinieerd in `scipy.special.erf`. Er is ook een complementaire error function, gedefinieerd als $\\mathrm{erfc} = 1 - \\mathrm{erf}$. Vergeet niet de factor $\\sqrt{2}$, aangezien $\\mathrm{erf}$ een vereenvoudigde functie is, en het argument nog omgerekend moet worden.\n",
    "\n",
    "De waarschijnlijkheid dat een waarde méér dan $1.23\\sigma$ van het gemiddelde afligt is gegeven door:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from scipy.special import erf, erfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erfc(1.23 / sqrt(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De waarschijnlijkheid dat een waarde méér dan $2.43\\sigma$ *boven* het gemiddelde ligt is gegeven door:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erfc(2.43 / sqrt(2)) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De waarschijnlijkheid dat een waarde meer dan $0.5\\sigma$, maar minder dan $1.5\\sigma$ van het gemiddelde verwijderd is, wordt gegeven door:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erf(1.5 / sqrt(2)) - erf(.5 / sqrt(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We willen weten binnen hoeveel standaarddeviaties van het gemiddelde de waarschijnlijkheid dat een waarde gevonden wordt gelijk is aan $50\\,\\%$. Gebruik hiervoor de inverse error function `erfinv`. Vergeet de factor $\\sqrt{2}$ niet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erfinv(.5) * sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opgave 1.4\n",
    "\n",
    "We maken gebruik van pandas, een python data-analyse pakket. We importeren de datafile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('10-metingen.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...en bekijken wat statistieken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De standaarddeviatie van het gemiddelde is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.x\n",
    "x.std() / sqrt(x.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het resultaat van ons experiment is dus $x = 71.8 \\pm 1.7$ of $x = 71.8(17)$.\n",
    "\n",
    "Voor De kans op een volgende meting met $x\\geq 75$ berekenen we eerst hoeveel standaarddeviaties de meting verwijderd is van het gemiddelde. Vervolgens berekenen we de kans op een meting hóger dan dat aantal standaarddeviaties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 75 - x.mean()\n",
    "dist_sigma = dist / x.std()\n",
    "erfc(dist_sigma / sqrt(2)) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opgave 1.5\n",
    "\n",
    "We gaan plotten, en we moeten `matplotlib` vertellen dat we de plots in de notebook willen zien:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We openen de datafile en maken een histogram van $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('80-metingen.txt')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.y.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het is best interessant dat dit histogram er heel anders uitziet dan het antwoord uit de uitwerkingen. Dat histogram is gemaakt met Origin, en dat lijkt standaard iets beter op zoek te gaan naar aardige bingrenzen. Niet simpelweg het minimum en het maximum nemen en opdelen, maar afronden op mooie, ronde getallen. Handmatig kan dat in Python. Voor stapgroottes kun je het beste `arange` gebruiken. Let er daarbij wel op dat `arange` niet inclusief is. Het maximum wordt niet meegenomen. Als je dat ietsje groter maakt dan de bingrens, dus 101 i.p.v. 100, dan komt het goed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bins = np.arange(20, 101, 10)\n",
    "data.y.hist(bins=bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methode A: fitten aan volledige dataset\n",
    "\n",
    "Het fitten van een bekende distributie (zie `scipy.stats`) aan de data gaat vrij eenvoudig. Daar heb je niet eens een histogram voor nodig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "mean, sigma = stats.norm.fit(data.y)\n",
    "mean, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De meest waarschijnlijke distributie heeft een gemiddelde van 61.6 en een standaarddeviatie van 16.7. Je krijgt alleen de distributie, genormeerd, dus als je dat samen met het histogram wilt plotten moet je eerst nog schalen met een factor $N^2 / N_\\mathrm{bins}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data.y.hist(bins=bins)\n",
    "\n",
    "scale = data.y.count() ** 2 / len(bins - 1)\n",
    "x = np.linspace(20, 100, 50)\n",
    "plt.plot(x, scale * stats.norm.pdf(x, mean, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het is dan wellicht interessanter om het histogram weer te geven als waardes met foutenvlaggen. Eerst wil je de waardes van het histogram, zonder te plotten. Dan moet je de middens van de bins berekenen, en dat plotten met de fouten $\\sqrt{N}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, _ = np.histogram(data.y, bins=bins)\n",
    "xbins = (bins[:-1] + bins[1:]) / 2\n",
    "plt.errorbar(xbins, n, yerr=np.sqrt(n), fmt='o')\n",
    "\n",
    "plt.plot(x, scale * stats.norm.pdf(x, mean, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merk op dat de waardes van de fit iets verschillen van die van Origin, helaas. De volledige code om het laatste plaatje te maken wordt dan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read datafile\n",
    "data = pd.read_csv('80-metingen.txt')\n",
    "\n",
    "# calculate histogram\n",
    "bins = np.arange(20, 101, 10)\n",
    "n, _ = np.histogram(data.y, bins=bins)\n",
    "\n",
    "# plot histogram with bin centers and error bars\n",
    "xbins = (bins[:-1] + bins[1:]) / 2\n",
    "plt.errorbar(xbins, n, yerr=np.sqrt(n), fmt='o')\n",
    "\n",
    "# fit normal distribution to data\n",
    "mean, sigma = stats.norm.fit(data.y)\n",
    "\n",
    "# plot the scaled distribution\n",
    "scale = data.y.count() ** 2 / len(bins - 1)\n",
    "x = np.linspace(20, 100, 50)\n",
    "plt.plot(x, scale * stats.norm.pdf(x, mean, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vertrouwen de data wel. De fit gaat door of vlak langs alle foutenvlaggen.\n",
    "\n",
    "Dit is wel meer werk dan in Origin, vooral als je nog niet zo handig bent. Zeker ook meer om te onthouden. **Merk op dat de fit gedaan is op de volledige set waarnemingen, dus niet op het histogram. Als er gefit moet worden aan een histogram, of als je een zelf-gedefinieerde functie wilt fitten, dan moet je overstappen naar least-squares fitting.**\n",
    "\n",
    "### Methode B: fitten aan een histogram\n",
    "\n",
    "Als je least-squares wilt fitten, gebruik dan de `lmfit` bibliotheek. Die is handiger dan `scipy` gebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmfit import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kunt veel verschillende modellen gebruiken, b.v. een `GaussianModel`. De werkwijze is als volgt: éérst initialiseer je het model en vervolgens voer je de fit uit. Het is handig om voor de fit een *first guess* op te geven, een afschatting van de parameters. Je kunt dit handmatig doen, maar wij gebruiken hier de method `guess` van het model. Dit geeft een afschatting van de parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss = models.GaussianModel()\n",
    "first_guess = gauss.guess(n, x=xbins)\n",
    "first_guess.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...die je vervolgens in de fit kunt stoppen. Denk hierbij aan de fout op de datapunten. In de fit kun je een *gewicht* toekennen, dat omgekeerd evenredig is met de fout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = gauss.fit(n, x=xbins, weights=1 / np.sqrt(n), params=first_guess)\n",
    "print(fit.fit_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.plot(numpoints=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergelijking van de methodes A en B\n",
    "\n",
    "Je kunt je nu afvragen welke methode beter is. Bovenstaande fit ziet er prachtig uit. Toch is er nog een subtiliteit: het histogram hangt nogal af van de keuze voor de bins. In bovenstaand voorbeeld hadden we de bins zelf gekozen. Een andere keuze voor de bins, levert een andere fit op:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins = np.histogram(data.y)\n",
    "xbins = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "first_guess = gauss.guess(n, x=xbins)\n",
    "fit = gauss.fit(n, x=xbins, weights=1 / np.sqrt(n), params=first_guess)\n",
    "print(fit.fit_report())\n",
    "fit.plot(numpoints=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze fit heeft een iets ander centrum, een andere breedte, en een fors lagere amplitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opgave 1.6\n",
    "\n",
    "We importeren de dataset en kijken even of dat goed ging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('verval.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zien een kolom **tijd** en een kolom **counts**, met negen meetpunten. De fouten op de meetpunten stonden niet in het bestand, maar wel in de opgave. De fout op het aantal counts $N$ wordt gegeven door $\\sqrt N$. We maken een nieuwe kolom `yerr` als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['yerr'] = np.sqrt(data.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merk op dat we de data nu kunnen bekijken door zowel `data['yerr']` als `data.yerr` te typen, maar dat we nieuwe kolommen alléén kunnen maken met de `data['yerr']`-notatie. We inspecteren even de eerste vijf regels om te zien of alles goed ging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maken een plot van counts tegen tijd, met de juiste foutvlaggen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.scatter('tijd', 'counts', yerr='yerr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De vraag is nu hoe we het beste om kunnen gaan met de achtergrond. Eenvoudig negeren, fitten aan een exponentiële functie plus een constante achtergrond of de achtergrond éérst aftrekken van de data? We bekijken hieronder de drie methodes.\n",
    "\n",
    "### Methode A: negeren van de achtergrond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.ExponentialModel()\n",
    "fit = model.fit(data.counts, x=data.tijd, weights=1 / data.yerr)\n",
    "print(fit.fit_report())\n",
    "fit.plot(numpoints=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Op het oog ziet dit er prima uit, maar $\\chi_\\mathrm{red}^2 = 1.659$, vrij groot. Aan de residuals is nog niet heel veel te zien. Je moet langer doormeten om duidelijk te kunnen zien dat de exponentiële functie naar nul gaat, terwijl de metingen dat niet doen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methode B: fitten aan exponentiële functie plus constante achtergrond\n",
    "\n",
    "We kunnen met `lmfit` eenvoudig modellen optellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.ExponentialModel() + models.ConstantModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vervolgens fitten we dit model aan de data en bekijken het resultaat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = model.fit(data.counts, x=data.tijd, weights=1 / data.yerr)\n",
    "print(fit.fit_report())\n",
    "fit.plot(numpoints=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dat is een mooi resultaat. Op het oog gaat de fit goed door de punten en $\\chi^2_\\mathrm{red} = 1.167$. Dit resultaat is waarschijnlijker dan het resultaat van methode A, waar we de achtergrond negeren.\n",
    "\n",
    "### Methode C: Achtergrond van de data aftrekken\n",
    "\n",
    "Uit de opgave weten we dat er wel degelijk een achtergrond is, en dat die bepaald is op gemiddeld 12 counts per minuut. We kunnen een nieuwe kolom maken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['nobkg'] = data.counts - 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De vraag is nu: wat doen we met de fout op deze data? Gebruiken we dezelfde fout als eerst? Maar dat was de wortel van het aantal counts mét achtergrond. De wortel van het aantal counts zónder achtergrond, dus `sqrt(data.nobkg)`, ligt voor de hand maar is niet juist. Niet elke meting had immers precies 12 counts aan achtergrond. We trekken 12 counts af, maar de verwachte onzekerheid op het *meten* van de achtergrond is $\\sqrt{12}$. Dat telt mee.\n",
    "\n",
    "We moeten de regels voor de foutenberekening aanroepen: $N_\\mathrm{nobkg} = N - N_\\mathrm{bkg}$. Dus de onzekerheid wordt gegeven door $\\delta N_\\mathrm{nobkg} = \\sqrt{(\\delta N)^2 + (\\delta N_\\mathrm{bkg})^2}$. De onzekerheid wordt, na aftrekken van de achtergrond, dus eigenlijk zelfs *groter*. Nu was de achtergrond gelukkig heel nauwkeurig bepaald, dus de onzekerheid op de grootte van de achtergrond is verwaarloosbaar klein. Dat betekent dat $\\delta N_\\mathrm{nobkg} = \\delta N = \\sqrt{N}$. We houden dus de oorspronkelijke onzekerheid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.ExponentialModel()\n",
    "fit = model.fit(data.nobkg, x=data.tijd, weights=1 / data.yerr)\n",
    "print(fit.fit_report())\n",
    "fit.plot(numpoints=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De $\\chi_\\mathrm{red}^2 = 1.005$. Dit is dus de meest nauwkeurige methode om de halfwaardetijd te bepalen. De halfwaardetijd wordt gegeven door $t_{1/2} = \\lambda\\ln 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.params['decay'].value * np.log(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dus $t_{1/2} = 1.196$ s. Het kan makkelijker zijn om een eigen model te maken, waar $t_{1/2}$ expliciet in voorkomt: $N(t) = N_0 * \\frac{1}{2}^{t/t_{1/2}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verval = models.Model(lambda t, N0, thalf: N0 * .5 ** (t / thalf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meestal moeten we een schatting opgeven van de parameters. Zonder die schatting is het goed mogelijk dat het fit-algoritme zoekt bij zulke onrealistische waardes dat er 'oneindig' uit de functie komt. Daarn kan het algoritme niet goed mee omgaan. We kiezen gewoon de waarde 1 voor beide parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = verval.make_params(N0=1, thalf=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = verval.fit(data.nobkg, t=data.tijd, weights=1 / data.yerr, params=guess)\n",
    "print(fit.fit_report())\n",
    "fit.plot(numpoints=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En we lezen nu meteen af dat $t_{1/2} = 1.196 \\pm 0.027$ s, ofwel $t_{1/2} = 1.196(27)$ s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
